#+TITLE: Deap Learning From Scratch
* 第1章　Python入门
** 1.1 Python是什么
** 1.2 Python的安装
*** 1.2.1　Python版本
本书中使用Python 3.x，
*** 1.2.2　使用的外部库
NumPy 是用于数值计算的库,提供了很多高级的数学算法和便利的数组(矩阵)操作方法。

Matplotlib 是用来画图的库。使用 Matplotlib 能将实验结果可视化,并在视觉上确
认深度学习运行期间的数据。
*** 1.2.3　Anaconda发行版
** 1.3 Python解释器
*** 1.3.1　算术计算
[[file:example/1-3-1.py]]
*** 1.3.2　数据类型
type()
[[file:example/1-3-2.py]]
*** 1.3.3　变量
[[file:example/1-3-3.py]]
*** 1.3.4　列表
[[file:example/1-3-4.py]]
*** 1.3.5　字典
[[file:example/1-3-5.py]]
*** 1.3.6　布尔型
[[file:example/1-3-6.py]]
*** 1.3.7　if 语句
[[file:example/1-3-7.py]]
*** 1.3.8　for 语句
[[file:example/1-3-8.py]]
*** 1.3.9　函数
[[file:example/1-3-9.py]]
** 1.4 Python脚本文件
*** 1.4.1　保存为文件
[[file:example/ch01/hungry.py]]
*** 1.4.2　类
[[file:example/ch01/man.py]]
** 1.5 NumPy　
*** 1.5.1　导入NumPy
import numpy as np
*** 1.5.2　生成NumPy数组　
numpy.array <class 'numpy.ndarray'>
[[file:example/1-5-2.py]]
*** 1.5.3　NumPy 的算术运算　
[[file:example/1-5-3.py]]
"对应元素的”的英文是 element-wise.
在 NumPy 数组的各个元素和标量之间进行运算。这个功能也被称为广播.
*** 1.5.4　NumPy的N维数组
矩阵形状可以通过 shape 查看.
矩阵元素的数据类型可以通过 dtype 查看.
数学上将一维数组称为向量,将二维数组称为矩阵。
可以将一般化之后的向量或矩阵等统称为张量 (tensor)。
[[file:example/1-5-4.py]]
*** 1.5.5　广播　
标量被扩展成了矩阵的形状,然后再与矩阵进行乘法运算。这个巧妙的功能称为广播
(broadcast)。

[[file:example/1-5-5.py]]
*** 1.5.6　访问元素　
[[file:example/1-5-6.py]]
** 1.6 Matplotlib　
*** ^
图形的绘制和数据的可视化
*** 1.6.1　绘制简单图形　
matplotlib 的 pyplot 模块绘制图形

[[file:example/ch01/sin_graph.py]]
*** 1.6.2　pyplot 的功能　
[[file:example/ch01/sin_cos_graph.py]]
*** 1.6.3　显示图像
[[file:example/ch01/img_show.py]]
** 1.7 小结
- python ::《Python 语言及其应用》
- numpy :: 《利用 Python 进行数据分析》
- website about numpy and matplotlib :: “Scipy Lecture Notes”

本章所学的内容:
- Python 是一种简单易记的编程语言。
- Python 是开源的,可以自由使用。
- 本书中将使用 Python 3.x 实现深度学习。
- 本书中将使用 NumPy 和 Matplotlib 这两种外部库。
- Python 有“解释器”和“脚本文件”两种运行模式。
- Python 能够将一系列处理集成为函数或类等模块。
- NumPy 中有很多用于操作多维数组的便捷方法。
* 第2章　感知机
** ^
感知机（perceptron）这一算法.
感知机也是作为神经网络（深度学习）的起源的算法。
** 2.1 感知机是什么　
感知机接收多个输入信号，输出一个信号。

[[file:image/%E6%9C%89%E4%B8%A4%E4%B8%AA%E8%BE%93%E5%85%A5%E7%9A%84%E6%84%9F%E7%9F%A5%E6%9C%BA.png][有两个输入的感知机.png]]

- x1、x2是输入信号。
- y是输出信号。
- w1、w2是权重。
- ○称为“神经元”或者“节点”。
- “神经元被激活”的阀值，用符号θ表示。


\begin{equation}
\LARGE
y =
\left\{
  \begin{array}{ll}
    0 & (w_{1}x_{1} + w_{2}x_{2} \leq \theta )
    \\
    1 & (w_{1}x_{1} + w_{2}x_{2} > \theta )
  \end{array}
\right.
\end{equation}

** 2.2 简单逻辑电路　
*** 2.2.1　与门(AND gate)
[[file:image/%E4%B8%8E%E9%97%A8%E7%9A%84%E7%9C%9F%E5%80%BC%E8%A1%A8.png][与门的真值表]]

用感知机来表示这个与门。有无数多个。
(w1, w2, θ) = (0.5, 0.5, 0.7)
(w1, w2, θ) = (0.5, 0.5, 0.8)
(w1, w2, θ) = (1.0, 1.0, 1.0)
...
*** 2.2.2　与非门(NAND(Not AND) gate)和或门
[[file:image/%E4%B8%8E%E9%9D%9E%E9%97%A8%E7%9A%84%E7%9C%9F%E5%80%BC%E8%A1%A8.png][与非门的真值表]]
(w1, w2, θ) = (−0.5, −0.5, −0.7)
...

[[file:image/%E6%88%96%E9%97%A8%E7%9A%84%E7%9C%9F%E5%80%BC%E8%A1%A8.png][或门的真值表]]

机器学习的课题就是将这个决定参数值的工作交由计算机自动进行。学习是确定
合适的参数的过程.
*** summary
相同构造的感知机，只需通过适当地调整参数的值，就可以变身为与门、与非门、或门
** 2.3 感知机的实现　
*** 2.3.1　简单的实现　
[[file:example/2-3-1.py]]
*** 2.3.2　导入权重和偏置　
\begin{equation}
\LARGE
y =
\left\{
  \begin{array}{ll}
    0 & ( b + w_{1}x_{1} + w_{2}x_{2} \leq 0 )
    \\
    1 & ( b + w_{1}x_{1} + w_{2}x_{2} > 0 )
  \end{array}
\right.
\end{equation}

b称为偏置。
*** 2.3.3　使用权重和偏置的实现　
[[file:example/ch02/and_gate.py]]
[[file:example/ch02/nand_gate.py]]
[[file:example/ch02/or_gate.py]]

- w1和w2是控制输入信号的重要性的参数
- 偏置是调整神经元被激活的容易程度的参数

与门、与非门、或门是具有相同构造的感知机，区别只在于权重参数的值.
** 2.4 感知机的局限性　
*** 2.4.1　异或门(XOR gate)
[[file:image/%E5%BC%82%E6%88%96%E9%97%A8%E7%9A%84%E7%9C%9F%E5%80%BC%E8%A1%A8.png][异或门的真值表]]
*** 2.4.2　线性和非线性　
曲线分割而成的空间称为非线性空间，由直线分割而成的空间称为线性空间
** 2.5 多层感知机　
*** 2.5.1　已有门电路的组合　
[[file:image/%E4%B8%8E%E9%97%A8%E3%80%81%E4%B8%8E%E9%9D%9E%E9%97%A8%E3%80%81%E6%88%96%E9%97%A8%E7%9A%84%E7%AC%A6%E5%8F%B7.png][与门、与非门、或门的符号]]
[[file:image/%E9%80%9A%E8%BF%87%E7%BB%84%E5%90%88%E4%B8%8E%E9%97%A8%E3%80%81%E4%B8%8E%E9%9D%9E%E9%97%A8%E3%80%81%E6%88%96%E9%97%A8%E5%AE%9E%E7%8E%B0%E5%BC%82%E6%88%96%E9%97%A8.png][通过组合与门、与非门、或门实现异或门]]
[[file:image/%E5%BC%82%E6%88%96%E9%97%A8%E7%9A%84%E7%9C%9F%E5%80%BC%E8%A1%A8.png][异或门的真值表]]
*** 2.5.2　异或门的实现　
[[file:example/ch02/xor_gate.py]]
叠加了多层的感知机也称为多层感知机（multi-layered perceptron）。
** 2.6 从与非门到计算机　
《计算机系统要素：从零开始构建现代计算机》。

感知机通过叠加层能够进行非线性的表示，理论上还可以表示计算机进行的处理。
** 2.7 小结
- 感知机是具有输入和输出的算法。给定一个输入后，将输出一个既定的值。
- 感知机将权重和偏置设定为参数。
- 使用感知机可以表示与门和或门等逻辑电路。
- 异或门无法通过单层感知机来表示。
- 使用2层感知机可以表示异或门。
- 单层感知机只能表示线性空间，而多层感知机可以表示非线性空间。
- 多层感知机（在理论上）可以表示计算机
* 第3章　神经网络
** ^
好消息，即便对于复杂的函数，感知机也隐含着能够表示它的可能性。
坏消息，设定权重的工作，现在还是由人工进行的。

神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。
** 3.1 从感知机到神经网络
*** 3.1.1　神经网络的例子
中间层有时也称为隐藏层。

[[file:image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BE%8B%E5%AD%90.png][神经网络的例子]]

神经网络就神经元的连接方式而言，与感知机并没有任何差异。
*** 3.1.2　复习感知机
[[file:image/%E6%98%8E%E7%A1%AE%E8%A1%A8%E7%A4%BA%E5%87%BA%E5%81%8F%E7%BD%AE.png][明确表示出偏置]]

\begin{equation}
\LARGE
y =
\left\{
  \begin{array}{ll}
    0 & ( b + w_{1}x_{1} + w_{2}x_{2} \leq 0 )
    \\
    1 & ( b + w_{1}x_{1} + w_{2}x_{2} > 0 )
  \end{array}
\right.
\end{equation}

\begin{equation}
\LARGE y = h( b + w_{1}x_{1} + w_{2}x_{2} )
\end{equation}

\begin{equation}
\LARGE
h(x) = 
       \left\{
         \begin{array}{ll}
           0 & (x \leq 0) \\
           1 & (x > 0)
         \end{array}
       \right.
\end{equation}
*** 3.1.3　激活函数登场（activation function）
h（x）函数会将输入信号的总和转换为输出信号，这种函数一般称为激活函数。

\begin{equation}
\LARGE a = b + w_{1}x_{1} + w_{2}x_{2}
\end{equation}

\begin{equation}
\LARGE y = h(a)
\end{equation}

[[file:image/%E6%98%8E%E7%A1%AE%E6%98%BE%E7%A4%BA%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B.png][明确显示激活函数的计算过程]]

“神经元”和“节点”两个术语的含义相同.

激活函数是连接感知机和神经网络的桥梁.

- “朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数的模型。
- “多层感知机”是指神经网络，即使用 sigmoid函数等平滑的激活函数的多层网络。
** 3.2 激活函数
*** ^
将激活函数从阶跃函数换成其他函数，就可以进入神经网络的世界。
*** 3.2.1　sigmoid 函数(sigmoid function)
\begin{equation}
\LARGE h(x) = \frac {1}{1 + exp(-x)}
\end{equation}

exp(−x)表示 $$ e^{-x} $$ 的意思。e是纳皮尔常数2.7182 ...。

h(1.0) = 0.731
h(2.0) = 0.880
*** 3.2.2　阶跃函数的实现
#+BEGIN_SRC python
def step_function(x):
 y = x > 0
 return y.astype(np.int)
#+END_SRC
*** 3.2.3　阶跃函数的图形
[[file:example/ch03/step_function.py]]
*** 3.2.4　sigmoid 函数的实现　
[[file:example/ch03/sigmoid.py]] 
*** 3.2.5　sigmoid 函数和阶跃函数的比较
[[file:image/%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0%E4%B8%8Esigmoid%E5%87%BD%E6%95%B0%EF%BC%88%E8%99%9A%E7%BA%BF%E6%98%AF%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0%EF%BC%89.png][阶跃函数与sigmoid函数（虚线是阶跃函数）]]

不同点：
- “平滑性”的不同
- 感知机中神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数
  值信号。

相同点：
- 当输入信号为重要信息时，阶跃函数和sigmoid函数都会输出较大的值；当输入信号
  为不重要的信息时，两者都输出较小的值。
- 不管输入信号有多小，或者有多大，输出信号的值都在0到1之间。

[[file:example/ch03/sig_step_compare.py]]
*** 3.2.6　非线性函数
阶跃函数和sigmoid函数还有其他共同点，就是两者均为非线性函数。sigmoid函数是
一条曲线，阶跃函数是一条像阶梯一样的折线。

神经网络的激活函数必须使用非线性函数。因为使用线性函数的话，加深神经网络
的层数就没有意义了。

h(x) = cx 
y(x) = h(h(h(x)))
y(x) = c × c × c × x
y(x) = ax（注意，a = c^3）

因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。
*** 3.2.7　ReLU函数(Rectified Linear Unit)
\begin{equation}
\LARGE
h(x) = 
       \left\{
         \begin{array}{ll}
           x & (x > 0) \\
           0 & (x \leq 0)
         \end{array}
       \right.
\end{equation}


[[file:image/ReLU%E5%87%BD%E6%95%B0.png][ReLU函数]]

[[file:example/ch03/relu.py]]
** 3.3 多维数组的运算
*** 3.3.1　多维数组
[[file:example/3-3-1.py]]

二维数组也称为矩阵（matrix）。数组的横向排列称为行（row），纵向排列称为列
（column）。
*** 3.3.2　矩阵乘法
[[file:image/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B9%98%E7%A7%AF%E7%9A%84%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95.png][矩阵的乘积的计算方法]]

矩阵的乘积是通过左边矩阵的行（横向）和右边矩阵的列（纵向）以对应元素的方式
相乘后再求和而得到的.

[[file:example/3-3-2.py]]

乘积也称为点积.

np.dot(A, B)和np.dot(B, A)的值可能不一样。

[[file:image/%E5%9C%A8%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B9%98%E7%A7%AF%E8%BF%90%E7%AE%97%E4%B8%AD%EF%BC%8C%E5%AF%B9%E5%BA%94%E7%BB%B4%E5%BA%A6%E7%9A%84%E5%85%83%E7%B4%A0%E4%B8%AA%E6%95%B0%E8%A6%81%E4%BF%9D%E6%8C%81%E4%B8%80%E8%87%B4.png][在矩阵的乘积运算中，对应维度的元素个数要保持一致]]

[[file:image/A%E6%98%AF%E4%BA%8C%E7%BB%B4%E7%9F%A9%E9%98%B5%E3%80%81B%E6%98%AF%E4%B8%80%E7%BB%B4%E6%95%B0%E7%BB%84%E6%97%B6%EF%BC%8C%E4%B9%9F%E8%A6%81%E4%BF%9D%E6%8C%81%E5%AF%B9%E5%BA%94%E7%BB%B4%E5%BA%A6%E7%9A%84%E5%85%83%E7%B4%A0%E4%B8%AA%E6%95%B0%E4%B8%80%E8%87%B4.png][A是二维矩阵、B是一维数组时，也要保持对应维度的元素个数一致]]

*** 3.3.3　神经网络的内积
[[file:image/%E9%80%9A%E8%BF%87%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B9%98%E7%A7%AF%E8%BF%9B%E8%A1%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%BF%90%E7%AE%97.png][通过矩阵的乘积进行神经网络的运算]]
[[file:example/3-3-3.py]]
** 3.4 3层神经网络的实现
*** ^
[[file:image/3%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png][3层神经网络]]
*** 3.4.1　符号确认
[[file:image/%E6%9D%83%E9%87%8D%E7%9A%84%E7%AC%A6%E5%8F%B7.png][权重的符号]]
*** 3.4.2　各层间信号传递的实现　
[[file:image/%E4%BB%8E%E8%BE%93%E5%85%A5%E5%B1%82%E5%88%B0%E7%AC%AC1%E5%B1%82%E7%9A%84%E4%BF%A1%E5%8F%B7%E4%BC%A0%E9%80%92.png][从输入层到第1层的信号传递]]

\begin{equation}
\LARGE
a_{1}^{(1)} = w_{11}^{(1)}x_{1} + w_{12}^{(1)}x_{2} + b_{1}^{(1)}
\end{equation}

\begin{equation}
\LARGE
A^{(1)} = XW^{(1)} + B^{(1)}
\end{equation}

\begin{equation}
\LARGE
A^{(1)} = \left(
            \begin{array}{ccc}
              a_{1}^{(1)} & a_{2}^{(1)} & a_{3}^{(1)}
            \end{array}
          \right)
\end{equation}

\begin{equation}
\LARGE
x = \left(
      \begin{array}{cc}
        x_{1} & x_{2}
      \end{array}
    \right)
\end{equation}

\begin{equation}
\LARGE
B^{(1)} = \left(
      \begin{array}{ccc}
        b_{1}^{(1)} & b_{2}^{(1)} & b_{3}^{(1)}
      \end{array}
    \right)
\end{equation}

\begin{equation}
\LARGE
W^{(1)} = \left(
      \begin{array}{ccc}
        w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\
        w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}
      \end{array}
    \right)
\end{equation}

[[file:example/3-4-2.py]]

[[file:image/%E4%BB%8E%E8%BE%93%E5%85%A5%E5%B1%82%E5%88%B0%E7%AC%AC1%E5%B1%82%E7%9A%84%E4%BF%A1%E5%8F%B7%E4%BC%A0%E9%80%922.png][从输入层到第1层的信号传递2]]

[[file:image/%E7%AC%AC1%E5%B1%82%E5%88%B0%E7%AC%AC2%E5%B1%82%E7%9A%84%E4%BF%A1%E5%8F%B7%E4%BC%A0%E9%80%92.png][第1层到第2层的信号传递]]

[[file:image/%E4%BB%8E%E7%AC%AC2%E5%B1%82%E5%88%B0%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84%E4%BF%A1%E5%8F%B7%E4%BC%A0%E9%80%92.png][从第2层到输出层的信号传递]]
*** 3.4.3　代码实现小结　
[[file:example/3-4-3.py]]
** 3.5 输出层的设计
*** ^
神经网络可以用在分类问题和回归问题上，不过需要根据情况改变输出层的激活函数。
回归问题用恒等函数，分类问题用softmax函数.
*** 3.5.1　恒等函数和softmax 函数　
恒等函数会将输入按原样输出，对于输入的信息，不加以任何改动地直接输出。

[[file:image/%E6%81%92%E7%AD%89%E5%87%BD%E6%95%B0.png][恒等函数]]

\begin{equation}
\LARGE y_{k} = \frac {exp(a_{k})} {\sum_{i=1}^n exp(a_{i})} 
\end{equation}

[[file:image/softmax%E5%87%BD%E6%95%B0.png][softmax函数.png]]

[[file:example/3-5-1.py]]
*** 3.5.2　实现softmax 函数时的注意事项
- 溢出问题

\begin{equation}
\LARGE 
y_{k} = \frac {exp(a_{k})} {\sum_{i=1}^n exp(a_{i})}
      = \frac {C exp(a_{k})} {C \sum_{i=1}^n exp(a_{i})}
\end{equation}

\begin{equation}
\LARGE 
= \frac {exp(a_{k} + \log{C})} {\sum_{i=1}^n exp(a_{i} + \log{C})}
= \frac {exp(a_{k} + C')} {\sum_{i=1}^n exp(a_{i} + C')}
\end{equation}

$$ C' $$可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值。

[[file:example/3-5-2.py]]
*** 3.5.3　softmax 函数的特征
softmax函数的输出是0.0到1.0之间的实数。并且，softmax函数的输出值的总和是1。
输出总和为1是softmax函数的一个重要性质。正因为有了这个性质，我们才可以把
softmax函数的输出解释为“概率”。

求解机器学习问题的步骤可以分为“学习”和“推理”两个阶段.
*** 3.5.4　输出层的神经元数量
对于分类问题,输出层的神经元数量一般设定为类别的数量.

[[file:image/%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84%E7%A5%9E%E7%BB%8F%E5%85%83%E5%AF%B9%E5%BA%94%E5%90%84%E4%B8%AA%E6%95%B0%E5%AD%97.png][输出层的神经元对应各个数字]]
** 3.6 手写数字识别
*** ^
神经网络解决问题时，需要首先使用训练数据（学习数据）进行权重参数的学习；
进行推理时，使用刚才学习到的参数，对输入数据进行分类。
*** 3.6.1　MNIST数据集
MNIST 数据集是由 0 到 9 的数字图像构成的。训练图像有6万张,测试图像有1万张.

[[file:image/MNIST%20%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E4%BE%8B%E5%AD%90.png][MNIST 图像数据集的例子]]

MNIST 的图像数据是 28 像素 × 28 像素的灰度图像(1 通道),各个像素的取值在 0
到 255 之间。每个图像数据都相应地标有“7” “2” “1”等标签。

[[file:example/dataset/mnist.py]]　
[[file:example/ch03/mnist_show.py]]

load_mnist(normalize=True,flatten=True, one_hot_label=False):
- return :: 以“( 训练图像 , 训练标签 ),( 测试图像,测试标签 )”的形式返回读
            入的 MNIST 数据。
- parameter
  - normalize 设置是否将输入图像正规化为 0.0~1.0 的值。
    - False :: 则输入图像的像素会保持原来的 0~255
  - flatten :: 是否展开输入图像(变成一维数组)。
    - False :: 则输入图像为 1 × 28 × 28 的三维数组;
    - True ::  则输入图像会保存为由 784 个元素构成的一维数组。
  - one_hot_label :: 设置是否将标签保存为 one-hot 表示(one-hot
                     representation)。one-hot 表示是仅正确解标签为 1,其余皆
                     为 0 的数组,就像 [0,0,1,0,0,0,0,0,0,0] 这样。
    - False 时,只是像 7 、 2 这样简单保存正确解标签;
    - True 时,标签则保存为 one-hot 表示。

Python有 pickle这个便利的功能。这个功能可以将程序运行中的对象保存为文件。
*** 3.6.2　神经网络的推理处理　
- 把数据限定到某个范围内的处理称为正规化 (normalization)。
- 对神经网络的输入数据进行某种既定的转换称为预处理 (pre-processing)。
  - 使数据整体以 0 为中心分布,或者进行正规化,把数据的延展控制在一定范围内
  - 还有将数据整体的分布形状均匀化的方法,即数据白化 (whitening)等

[[file:example/ch03/neuralnet_mnist.py]]
*** 3.6.3　批处理
[[file:image/%E6%95%B0%E7%BB%84%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%8F%98%E5%8C%96.png][数组形状的变化]]
[[file:image/%E6%89%B9%E5%A4%84%E7%90%86%E4%B8%AD%E6%95%B0%E7%BB%84%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%8F%98%E5%8C%96.png][批处理中数组形状的变化]]

打包式的输入数据称为批 (batch)

批处理一次性计算大型数组要比分开逐步计算各个小型数组速度更快。

[[file:example/ch03/neuralnet_mnist_batch.py]]
** 3.7 小结
本章介绍了神经网络的前向传播。本章介绍的神经网络和上一章的感知机在信号的按
层传递这一点上是相同的,但是,向下一个神经元发送信号时,改变信号的激活函数有很
大差异。神经网络中使用的是平滑变化的 sigmoid函数,而感知机中使用的是信号急剧
变化的阶跃函数。

本章所学的内容
- 神经网络中的激活函数使用平滑变化的 sigmoid 函数或 ReLU 函数。
- 通过巧妙地使用 NumPy 多维数组,可以高效地实现神经网络。
- 机器学习的问题大体上可以分为回归问题和分类问题。
- 关于输出层的激活函数,回归问题中一般用恒等函数,分类问题中一般用softmax函数。
- 分类问题中,输出层的神经元的数量设置为要分类的类别数。
- 输入数据的集合称为批。通过以批为单位进行推理处理,能够实现高速的运算。
* 第4章　神经网络的学习
** ^
神经网络的学习的目的就是以损失函数为基准,找出能使它的值达到最小的权重参数。
** 4.1 从数据中学习
*** ^
由数据自动决定权重参数的值.
*** 4.1.1　数据驱动
数据是机器学习的核心。

机器学习的方法极力避免人为介入,尝试从收集到的数据中发现答案(模式)。

神经网络或深度学习则比以往的机器学习方法更能避免人为介入。

[[file:image/%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%205%20%E7%9A%84%E4%BE%8B%E5%AD%90:%E5%86%99%E6%B3%95%E5%9B%A0%E4%BA%BA%E8%80%8C%E5%BC%82,%E4%BA%94%E8%8A%B1%E5%85%AB%E9%97%A8.png][手写数字 5 的例子:写法因人而异,五花八门]]

从图像中提取特征量,再用机器学习技术学习这些特征量的模式。

“特征量”是指可以从输入数据(输入图像)中准确地提取本质数据(重要的数据)的转换器。

[[file:image/%E4%BB%8E%E4%BA%BA%E5%B7%A5%E8%AE%BE%E8%AE%A1%E8%A7%84%E5%88%99%E8%BD%AC%E5%8F%98%E4%B8%BA%E7%94%B1%E6%9C%BA%E5%99%A8%E4%BB%8E%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%AD%A6%E4%B9%A0:%E6%B2%A1%E6%9C%89%E4%BA%BA%E4%B8%BA%E4%BB%8B%E5%85%A5%E7%9A%84%E6%96%B9%E5%9D%97%E7%94%A8%E7%81%B0%E8%89%B2%E8%A1%A8%E7%A4%BA.png][从人工设计规则转变为由机器从数据中学习:没有人为介入的方块用灰色表示]]

深度学习有时也称为端到端机器学习(end-to-end machine learning)。就是从原始数
据(输入)中获得目标结果(输出)的意思。

神经网络的优点是对所有的问题都可以用同样的流程来解决
*** 4.1.2　训练数据和测试数据
机器学习中,一般将数据分为
- 训练数据（监督数据）
- 测试数据
两部分来进行学习和实验。为什么需要将数据分为训练数据和测试数据呢?因为我们追
求的是模型的泛化能力。

泛化能力是指处理未被观察过的数据(不包含在训练数据中的数据)的能力。获得泛化
能力是机器学习的最终目标。

只对某个数据集过度拟合的状态称为过拟合 (over fitting)。避免过拟合也是机器学
习的一个重要课题。
** 4.2 损失函数
*** ^
神经网络的学习通过某个指标(损失函数（loss function）)表示现在的状态。然后,
以这个指标为基准,寻找最优权重参数。

损失函数：
- 均方误差
- 交叉熵误差
- 等（可以为任意函数）

损失函数是表示神经网络性能的“恶劣程度”的指标。
*** 4.2.1　均方误差(mean squared error)
\begin{equation}
\LARGE
E = \frac {1} {2}
\sum_{k}(y_{k} - t_{k})^{2}
\end{equation}
- $$ y_{k} $$ :: 神经网络的输出
- $$ t_{k} $$ :: 监督数据
- k :: 表示数据的维数

[[file:example/ch04/4-2-1.py]]
*** 4.2.2　交叉熵误差(cross entropy error)
\begin{equation}
\LARGE
E = - \sum_{k} t_{k} \log{y_{k}}
\end{equation}

- log 表示以 e 为底数的自然对数($$ \log_{e}  $$)
- $$ y_{k} $$  是神经网络的输出
- $$ t_{k} $$ 是正确解标签。并且, 只有正确解标签的索引为 1,其他均为 0 (one-hot 表示).

[[file:image/%E8%87%AA%E7%84%B6%E5%AF%B9%E6%95%B0%20y%20=%20log%20x%20%E7%9A%84%E5%9B%BE%E5%83%8F.png][自然对数 y = log x 的图像]]

[[file:example/ch04/4-2-2.py]]
*** 4.2.3　mini-batch 学习
\begin{equation}
\LARGE
E = - \frac {1}{N} \sum_{n} \sum_{k} t_{nk} \log{y_{nk}}
\end{equation}

$$ t_{nk} $$ 表示第n个数据的第k个元素的值,除以N进行正规化。

神经网络的学习也是从训练数据中选出一批数据(称为 mini-batch, 小批量).

[[file:example/ch04/4-2-3.py]]
*** 4.2.4　mini-batch 版交叉熵误差的实现
[[file:example/ch04/4-2-4.py]]

由于one-hot表示中t为0的元素的交叉熵误差也为0,因此针对这些元素的计算可以忽略。
换言之,如果可以获得神经网络在正确解标签处的输出,就可以计算交叉熵误差。不是
正确解的输出是0。

[[file:example/ch04/4-2-4_2.py]]　
*** 4.2.5　为何要设定损失函数
在进行神经网络的学习时,不能将识别精度作为指标。因为如果以识别精度为指标,则
参数的导数在绝大多数地方都会变为 0。

[[file:image/%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0%E5%92%8C%20sigmoid%20%E5%87%BD%E6%95%B0.png][阶跃函数和 sigmoid 函数]]

阶跃函数的斜率在绝大多数地方都为 0,而 sigmoid 函数的斜率(切线)不会为 0.
** 4.3 数值微分(数值梯度)(numerical differentiation)
   梯度法使用梯度的信息决定前进的方向
*** 4.3.1　导数(derivative)
导数表示的是某个瞬间的变化量.

\begin{equation}
\LARGE
\frac {df(x)}{dx} = \lim_{h \rightarrow 0} \frac {f(x+h)-f(x)} {h}
\end{equation}

左边的符号表示f(x)关于x的导数,即f(x)相对于x的变化程度。表示的导数的含义是,x
的“微小变化”将导致函数f(x)的值在多大程度上发生变化。其中,表示微小变化的h
无限趋近0.

#+BEGIN_SRC python
# 不好的实现示例
def numerical_diff(f, x):
    h = 10e-50
    return (f(x+h) - f(x)) / h
#+END_SRC

两处需要改进的地方：
1. 舍入误差 :: (rounding error)是指因省略小数的精细部分的数值(比如,小数点第
           8 位以后的数值)而造成最终的计算结果上的误差.
2. 函数f的差分 :: 差分（difference）又名差分函数或差分运算，差分的结果反映
             了离散量之间的一种变化，是研究离散数学的一种工具。
   - 中心差分：函数f在(x+h)和(x−h)之间的差分.
   - 前向差分:(x+h)和x之间的差分.

[[file:image/%E7%9C%9F%E7%9A%84%E5%AF%BC%E6%95%B0(%E7%9C%9F%E7%9A%84%E5%88%87%E7%BA%BF)%E5%92%8C%E6%95%B0%E5%80%BC%E5%BE%AE%E5%88%86(%E8%BF%91%E4%BC%BC%E5%88%87%E7%BA%BF)%E7%9A%84%E5%80%BC%E4%B8%8D%E5%90%8C.png][真的导数(真的切线)和数值微分(近似切线)的值不同]]

#+BEGIN_SRC python
# 修改的例子
def numerical_diff(f, x):
    h = 1e-4 # 0.0001
    return (f(x+h) - f(x-h)) / (2*h)
#+END_SRC

利用微小的差分求导数的过程称为数值微分 (numerical differentiation)。而基于
数学式的推导求导数的过程,则用“解析性” (analytic)一词,称为“解析性求解”或
者“解析性求导”。

[[file:example/ch04/4-3-1.py]]
*** 4.3.2　数值微分的例子
[[file:example/ch04/gradient_1d.py]]

#+BEGIN_SRC python
def tangent_line(f, x):
    d = numerical_diff(f, x)
    print(d)

    y = f(x) - d*x
    return lambda t: d*t + y
#+END_SRC
d 斜率
点斜式方程： ty-y'= d(t-x') 
x' y' 为已知点
ty = y' + d(t-x')
y' = f(x)
y = f(x) + dt - dx'
dx' = dx
y = f(x) + dt - dx
*** 4.3.3　偏导数
\begin{equation}
\Large
f(x_{0}, x_{1}) = x_{0}^{2} + x_{1}^{2}
\end{equation}

#+BEGIN_SRC python
  def function_2(x):
      return x[0]**2 + x[1]**2
      # 或者 return np.sum(x**2)
#+end_src

[[file:image/x0^2+x1^2%E7%9A%84%E5%9B%BE%E5%83%8F.png][x1^2+x2^2的图像]]

有多个变量的函数的导数称为偏导数.用数学式表示的话,可以写成
\begin{equation}
\Large
\frac{\partial f}{\partial x_{0}}
\:\:\:\:
\frac{\partial f}{\partial x_{1}} 
\end{equation}

#+BEGIN_SRC python
  def function_tmp1(x0):
      return x0*x0 + 4.0**2.0

  numerical_diff(function_tmp1, 3.0)


  def function_tmp2(x1):
      return 3.0**2.0 + x1*x1

  numerical_diff(function_tmp2, 4.0)
#+END_SRC

偏导数和单变量的导数一样,都是求某个地方的斜率。不过,
偏导数需要将多个变量中的某一个变量定为目标变量,并将其他变量固定为
某个值。在上例的代码中,为了将目标变量以外的变量固定到某些特定的值
上,我们定义了新函数。然后,对新定义的函数应用了之前的求数值微分的
函数,得到偏导数。
** 4.4 梯度(gradient)
*** ^
像 
\begin{equation}
\Large
(
\frac{\partial f}{\partial x_{0}}
,
\frac{\partial f}{\partial x_{1}} 
)
\end{equation}
这样的由全部变量的偏导数汇总而成的向量称为梯度.

[[file:example/ch04/4-4.py]]
[[file:example/ch04/gradient_2d.py]]

梯度指示的方向是各点处的函数值减小最多的方向.

方向导数 = cos(θ) × 梯度(θ 是方向导数的方向与梯度方向的夹角).因此,所有的
下降方向中,梯度方向下降最多
*** 4.4.1　梯度法
通过巧妙地使用梯度来寻找函数最小值(或者尽可能小的值)的方法就是梯度法。

这里需要注意的是,梯度表示的是各点处的函数值减小最多的方向。因此,无法保证梯
度所指的方向就是函数的最小值或者真正应该前进的方向.

函数的极小值、最小值以及被称为鞍点 (saddle point)的地方,梯度为 0。
- 极小值是局部最小值,也就是限定在某个范围内的最小值。
- 鞍点是从某个方向上看是极大值,从另一个方向上看则是极小值的点。
虽然梯度法是要寻找梯度为 0 的地方,但是那个地方不一定就是最小值(也有可能是极
小值或者鞍点)。此外,当函数很复杂且呈扁平状时,学习可能会进入一个(几乎)平坦的
地区,陷入被称为“学习高原”的无法前进的停滞期。

通过不断地沿梯度方向前进,逐渐减小函数值的过程就是梯度法 (gradient method)。

寻找最小值的梯度法称为梯度下降法 (gradient descent method),寻找最大值的梯度
法称为梯度上升法 (gradient ascent method)。但是通过反转损失函数的符号,求最
小值的问题和求最大值的问题会变成相同的问题.

\begin{equation}
\Large
x_{0} = x_{0} - \eta \frac{\partial f}{\partial x_{0}}
\end{equation}
\begin{equation}
\Large
x_{1} = x_{1} - \eta \frac{\partial f}{\partial x_{1}}
\end{equation}

η表示更新量,在神经网络的学习中,称为学习率 (learning rate)。学习率决定在一
次学习中,应该学习多少,以及在多大程度上更新参数

学习率需要事先确定为某个值。一般而言,这个值过大或过小,都无法抵达一个“好的
位置”。

#+BEGIN_SRC python
  def gradient_descent(f, init_x, lr=0.01, step_num=100):
      x = init_x

      for i in range(step_num):
          grad = numerical_gradient(f, x)
          x -= lr * grad

      return x
#+END_SRC

[[file:example/ch04/gradient_method.py]]

像学习率这样的参数称为超参数。这是一种和神经网络的参数(权重和偏置)性质不同
的参数。相对于神经网络的权重参数是通过训练数据和学习算法自动获得的,学习率这
样的超参数则是人工设定的。一般来说,超参数需要尝试多个值,以便找到一种可以使
学习顺利进行的设定。
*** 4.4.2　神经网络的梯度
这里所说的梯度是指损失函数关于权重参数的梯度.

\begin{equation}
\Large
W =
\left(
  \begin{array}{lll}
    w_{11} & w_{12} & w_{13} \\
    w_{21} & w_{22} & w_{23}
  \end{array}
\right)
\end{equation}

\begin{equation}
\Large
\frac{\partial L}{\partial W} =
\left(
  \begin{array}{lll}
    \frac{\partial L}{\partial w_{11}} & 
    \frac{\partial L}{\partial w_{12}} & 
    \frac{\partial L}{\partial w_{13}} \\ \\
    \frac{\partial L}{\partial w_{21}} & 
    \frac{\partial L}{\partial w_{22}} & 
    \frac{\partial L}{\partial w_{23}}
  \end{array}
\right)
\end{equation}

L 为损失函数
W 为权重

[[file:example/ch04/gradient_simplenet.py]]
** 4.5 学习算法的实现
*** ^
- 前提 :: 神经网络存在合适的权重和偏置,调整权重和偏置以便拟合训练数据的过程
        称为“学习”。神经网络的学习分成下面 4 个步骤。
  - 步骤 1 (mini-batch) :: 从训练数据中随机选出一部分数据,这部分数据称为
       mini-batch。我们的目标是减小 mini-batch 的损失函数的值。
  - 步骤 2 (计算梯度) :: 为了减小 mini-batch 的损失函数的值,需要求出各个权
                   重参数的梯度。梯度表示损失函数的值减小最多的方向。
  - 步骤 3 (更新参数) :: 将权重参数沿梯度方向进行微小更新。
  - 步骤 4 (重复) :: 重复步骤 1、步骤 2、步骤 3。

使用的数据是随机选择的 mini batch 数据,所以又称为随机梯度下降法 (stochastic
gradient descent)(SGD)。
*** 4.5.1　2 层神经网络的类
[[file:example/ch04/two_layer_net.py]]
*** 4.5.2　mini-batch 的实现
[[file:example/ch04/train_neuralnet.py]]
[[file:example/ch04/4-5-2.py]]
[[file:image/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E6%8E%A8%E7%A7%BB:%E5%B7%A6%E5%9B%BE%E6%98%AF%2010000%20%E6%AC%A1%E5%BE%AA%E7%8E%AF%E7%9A%84%E6%8E%A8%E7%A7%BB,%E5%8F%B3%E5%9B%BE%E6%98%AF%201000%20%E6%AC%A1%E5%BE%AA%E7%8E%AF%E7%9A%84%E6%8E%A8%E7%A7%BB.png][损失函数的推移:左图是 10000 次循环的推移,右图是 1000 次]]
*** 4.5.3　基于测试数据的评价
epoch 是一个单位。一个 epoch 表示学习中所有训练数据均被使用过一次时的更新次
数。比如,对于 10000 笔训练数据,用大小为 100笔数据的 mini-batch 进行学习时,
重复随机梯度下降法 100 次,所有的训练数据就都被“看过”了 A 。此时,100 次就
是一个 epoch。

[[file:example/ch04/train_neuralnet.py]]
** 4.6 小结
• 机器学习中使用的数据集分为训练数据和测试数据。
• 神经网络用训练数据进行学习,并用测试数据评价学习到的模型的
泛化能力。
• 神经网络的学习以损失函数为指标,更新权重参数,以使损失函数
的值减小。
• 利用某个给定的微小值的差分求导数的过程,称为数值微分。
• 利用数值微分,可以计算权重参数的梯度。
• 数值微分虽然费时间,但是实现起来很简单。
* 第5章　误差反向传播法
** 5.1 计算图
*** 5.1.1　用计算图求解
*** 5.1.2　局部计算
*** 5.1.3　为何用计算图解题
** 5.2 链式法则
*** 5.2.1　计算图的反向传播
*** 5.2.2　什么是链式法则
*** 5.2.3　链式法则和计算图
** 5.3 反向传播
*** 5.3.1　加法节点的反向传播
*** 5.3.2　乘法节点的反向传播
*** 5.3.3　苹果的例子
** 5.4 简单层的实现
*** 5.4.1　乘法层的实现
*** 5.4.2　加法层的实现
** 5.5 激活函数层的实现
*** 5.5.1　ReLU层
*** 5.5.2　Sigmoid 层
** 5.6 AffineSoftmax层的实现
*** 5.6.1　Affine层
*** 5.6.2　批版本的Affine层
*** 5.6.3　Softmax-with-Loss 层
** 5.7 误差反向传播法的实现
*** 5.7.1　神经网络学习的全貌图
*** 5.7.2　对应误差反向传播法的神经网络的实现
*** 5.7.3　误差反向传播法的梯度确认
*** 5.7.4　使用误差反向传播法的学习
** 5.8 小结
* 第6章　与学习相关的技巧
** 6.1 参数的更新
*** 6.1.1　探险家的故事
*** 6.1.2　SGD
*** 6.1.3　SGD的缺点
*** 6.1.4　Momentum
*** 6.1.5　AdaGrad
*** 6.1.6　Adam
*** 6.1.7　使用哪种更新方法呢
*** 6.1.8　基于MNIST数据集的更新方法的比较
** 6.2 权重的初始值
*** 6.2.1　可以将权重初始值设为0 吗
*** 6.2.2　隐藏层的激活值的分布
*** 6.2.3　ReLU的权重初始值
*** 6.2.4　基于MNIST数据集的权重初始值的比较
** 6.3 Batch Normalization
*** 6.3.1　Batch Normalization 的算法
*** 6.3.2　Batch Normalization 的评估
** 6.4 正则化
*** 6.4.1　过拟合
*** 6.4.2　权值衰减
*** 6.4.3　Dropout
** 6.5 超参数的验证
*** 6.5.1　验证数据
*** 6.5.2　超参数的最优化
*** 6.5.3　超参数最优化的实现
** 6.6 小结
* 第7章　卷积神经网络
** 7.1 整体结构
** 7.2 卷积层
*** 7.2.1　全连接层存在的问题
*** 7.2.2　卷积运算
*** 7.2.3　填充
*** 7.2.4　步幅
*** 7.2.5　3 维数据的卷积运算
*** 7.2.6　结合方块思考
*** 7.2.7　批处理
** 7.3 池化层
** 7.4 卷积层和池化层的实现
*** 7.4.1　4 维数组
*** 7.4.2　基于im2col 的展开
*** 7.4.3　卷积层的实现
*** 7.4.4　池化层的实现
** 7.5 CNN的实现
** 7.6 CNN的可视化
*** 7.6.1　第1 层权重的可视化
*** 7.6.2　基于分层结构的信息提取
** 7.7 具有代表性的CNN
*** 7.7.1　LeNet
*** 7.7.2　AlexNet
** 7.8 小结
* 第8章　深度学习
** 8.1 加深网络
*** 8.1.1　向更深的网络出发
*** 8.1.2　进一步提高识别精度
*** 8.1.3　加深层的动机
** 8.2 深度学习的小历史
*** 8.2.1　ImageNet
*** 8.2.2　VGG
*** 8.2.3　GoogLeNet
*** 8.2.4　ResNet
** 8.3 深度学习的高速化
*** 8.3.1　需要努力解决的问题
*** 8.3.2　基于GPU的高速化
*** 8.3.3　分布式学习
*** 8.3.4　运算精度的位数缩减
** 8.4 深度学习的应用案例
*** 8.4.1　物体检测
*** 8.4.2　图像分割
*** 8.4.3　图像标题的生成
** 8.5 深度学习的未来
*** 8.5.1　图像风格变换
*** 8.5.2　图像的生成
*** 8.5.3　自动驾驶
*** 8.5.4　Deep Q-Network（强化学习）
** 8.6 小结
* 附录A　Softmax-with-Loss 层的计算图
** A.1 正向传播
** A.2 反向传播
** A.3 小结
